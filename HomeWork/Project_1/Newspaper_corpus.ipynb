{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Газетный корпус\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала, необходимо выкачать статьи с сайта газеты\n",
    "В моем случае, это газета [\"Уездные вести\"](http://smi67.ru/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import csv\n",
    "\n",
    "def get_links(pageUrl):\n",
    "    try:\n",
    "        page = urllib.request.urlopen(pageUrl)  # Считывает код страницы\n",
    "        text = page.read().decode('utf-8')\n",
    "    except:\n",
    "        print('Error at', pageUrl)\n",
    "    articles_links = re.findall(r'class=\"entry-title\"><a href=\"(.*?)\" rel=\"bookmark\">', text)  # Ищет на странице ссылки на статьи\n",
    "    return articles_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующая функция очень сложная, но я объясню"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_page(pageUrl, path):\n",
    "    try:\n",
    "        page = urllib.request.urlopen(pageUrl)\n",
    "        text = page.read().decode('utf-8')\n",
    "\n",
    "        regPostcontent_1 = re.compile('<p style=\"text-align: justify;\">.*?<hr>', flags=re.DOTALL) #Регулярка для поиска текста\n",
    "        regPostcontent_2 = re.compile('<p style=\"text-align: justify;\">.*?<hr />', flags=re.DOTALL) #Второй вариант,встречается в некоторых статьях\n",
    "        content = regPostcontent_1.findall(text)\n",
    "        if len(content) == 0:\n",
    "            content = regPostcontent_2.findall(text) \n",
    "\n",
    "        if len(content) > 0: #Проверка, действительно ли нашелся текст\n",
    "            new_content = []\n",
    "            regTag2 = re.compile('<.*?>', re.DOTALL)\n",
    "            regSpace2 = re.compile('\\s{2,}', re.DOTALL)\n",
    "            regDescr2 = re.compile('\\n', re.DOTALL)\n",
    "            regSimbols = re.compile('&#\\d*?;', re.DOTALL)\n",
    "            for c in content: #Чистим текст от ненужных знаков и тэгов\n",
    "                clean_c = regSpace2.sub(\"\", c)\n",
    "                clean_c = regTag2.sub(\"\", clean_c)\n",
    "                clean_c = regDescr2.sub(\"\", clean_c)\n",
    "                clean_c = regSimbols.sub(\"\", clean_c)\n",
    "                new_content.append(clean_c)\n",
    "            for c in new_content:\n",
    "                c_clean = c.replace(\"&nbsp;\", \" \")\n",
    "\n",
    "            new_title = []\n",
    "            titles = re.findall('<h1 class=\"entry-title\">(.*?)</h1>', text) #Поиск заголовка\n",
    "            for t in titles: #Иногда попадаются лишние символы, поэтому почистим\n",
    "                clean_t = regSpace2.sub(\"\", t)\n",
    "                clean_t = regTag2.sub(\"\", clean_t)\n",
    "                clean_t = regDescr2.sub(\"\", clean_t)\n",
    "                clean_t = regSimbols.sub(\"\", clean_t)\n",
    "                new_title.append(clean_t)\n",
    "\n",
    "            author = re.findall(r'<span class=\"author vcard\"><a class=\".*?\" href=\"http://smi67.ru/author/.*?\">(.*?)</a></span>', text) #Поиск автора\n",
    "            data_time = re.findall(r'<time class=\"entry-date published\" datetime=\".*?\">(.*?)</time>', text)\n",
    "            if len(data_time) == 0: \n",
    "                data_time = re.findall(r'<time class=\"entry-date published updated\" datetime=\".*?\">(.*?)</time>', text) #Второй вариант для поиска даты\n",
    "\n",
    "            all = '@au' + ' ' + author[0] + '\\n' + '@ti' + ' ' + new_title[0] + '\\n' + '@da' + ' ' + data_time[0] + '\\n' + '@url' + ' ' + pageUrl + '\\n' +c_clean\n",
    "\n",
    "            with open(path, 'w', encoding='utf-8') as f:\n",
    "                f.write(all) \n",
    "            with open('newspaper\\metadata.csv', mode='a', encoding=\"utf-8\") as csv_file:\n",
    "                fieldnames = ['path', 'author', 'sex', 'birthday', 'header', 'created', 'sphere', 'genre_fi', 'type',\n",
    "                              'topic', 'chronotop', 'style', 'audience_age', 'audience_level', 'audience_size',\n",
    "                              'source', 'publication', 'publisher', 'publ_year', 'medium', 'country', 'region',\n",
    "                              'language']\n",
    "\n",
    "                #writer.writeheader() - Для первого запуска, для написания названий колонок\n",
    "                writer = csv.DictWriter(csv_file, fieldnames=fieldnames, delimiter='\\t')\n",
    "\n",
    "                writer.writerow({'author': author[0],\n",
    "                                 'header': titles[0],\n",
    "                                 'created': data_time[0],\n",
    "                                 'sphere': 'публицистика',\n",
    "                                 'style': 'нейтральный',\n",
    "                                 'audience_age': 'н-возраст',\n",
    "                                 'audience_level': 'н-уровень',\n",
    "                                 'audience_size': 'районная',\n",
    "                                 'source': pageUrl,\n",
    "                                 'publication': 'Уездные вести'})\n",
    "    except:\n",
    "        print('Error at', pageUrl)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    commonUrl = 'http://smi67.ru/date/'\n",
    "    months = ['01/', '02/', '03/', '04/', '05/', '06/', '07/', '08/', '09/', '10/'] #Номера месяцев\n",
    "\n",
    "    all_links_2018 = []\n",
    "    all_links_2017 = []\n",
    "    year = ['2018', '2017']\n",
    "    for y in year: #Статьи за год\n",
    "        for m in months: #Статьи за месяц\n",
    "            pageUrl_raw = commonUrl + y +'/' + m # За каждый месяц - несколько страницы со статьями\n",
    "            for i in range(2, 7):\n",
    "                pageUrl = pageUrl_raw + 'page/' + str(i) + '/'   \n",
    "                for links in get_links(pageUrl):\n",
    "                    if y == '2018':\n",
    "                        all_links_2018.append(links)\n",
    "                    else:\n",
    "                        all_links_2017.append(links)\n",
    "    m = 0\n",
    "    for links in all_links_2018: #Статьи за 2018 год\n",
    "        path_2018 = 'newspaper\\plain\\2018\\article_' + str(m) +'.txt' #Путь к статьям за 2018 год\n",
    "        #print(path_2018)\n",
    "        download_page(links, path_2018)\n",
    "        m = m + 1\n",
    "    n = 0\n",
    "    for links in all_links_2017: #Статьи за 2017 год\n",
    "        path_2017 = 'newspaper\\plain\\2017\\article_' + str(n) +'.txt' #Путь к статьям за 2018 год\n",
    "        #print(path_2017)\n",
    "        download_page(links, path_2017)\n",
    "        n = n + 1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующая часть программы нацелена на mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "articles_2018_list = os.listdir(r'C:\\mystem-3.1-win-64bit\\newspaper\\plain\\2018')\n",
    "articles_2017_list = os.listdir(r'C:\\mystem-3.1-win-64bit\\newspaper\\plain\\2017')\n",
    "for article_name in articles_2018_list:\n",
    "    raw_path_1 = \"C:\\mystem-3.1-win-64bit\\mystem.exe --eng-gr -nlcgid %s %s\"\n",
    "    raw_path_2 = \"C:\\mystem-3.1-win-64bit\\mystem.exe --eng-gr -nlcgid --format xml %s %s\"\n",
    "    path_plain_articles_1 = raw_path_1 % (r'C:\\mystem-3.1-win-64bit\\newspaper\\plain\\2018' + '\\\\' + article_name, r'C:\\mystem-3.1-win-64bit\\newspaper\\mystem-plain\\2018' + '\\\\' + article_name)\n",
    "    path_plain_articles_2 = raw_path_2 % (r'C:\\mystem-3.1-win-64bit\\newspaper\\plain\\2018' + '\\\\' + article_name, r'C:\\mystem-3.1-win-64bit\\newspaper\\mystem-xml\\2018' + '\\\\' + article_name.replace('txt', 'xml'))\n",
    "    print(path_plain_articles_1)\n",
    "    print(path_plain_articles_2)\n",
    "    run_mystem = os.system(path_plain_articles_1)\n",
    "    run_mustem_xml = os.system(path_plain_articles_2)\n",
    "for article_name in articles_2017_list:\n",
    "    raw_path_1 = \"C:\\mystem-3.1-win-64bit\\mystem.exe --eng-gr -nlcgid %s %s\"\n",
    "    raw_path_2 = \"C:\\mystem-3.1-win-64bit\\mystem.exe --eng-gr -nlcgid --format xml %s %s\"\n",
    "    path_plain_articles_1 = raw_path_1 % (r'C:\\mystem-3.1-win-64bit\\newspaper\\plain\\2017' + '\\\\' + article_name, r'C:\\mystem-3.1-win-64bit\\newspaper\\mystem-plain\\2017' + '\\\\' + article_name)\n",
    "    path_plain_articles_2 = raw_path_2 % (r'C:\\mystem-3.1-win-64bit\\newspaper\\plain\\2017' + '\\\\' + article_name, r'C:\\mystem-3.1-win-64bit\\newspaper\\mystem-xml\\2017' + '\\\\' + article_name.replace('txt', 'xml'))\n",
    "    run_mystem = os.system(path_plain_articles_1)\n",
    "    run_mustem_xml = os.system(path_plain_articles_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
